\documentclass{article}

\usepackage{nips_like}
\usepackage{config}


\title{Variational Inference}
\author{
    Xingdong Zuo\thanks{\today} \\
    %Affiliations \\
    %% \And or \AND
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
}

\begin{document}

\maketitle

Let $\bfX = \set{\bfx_1, \dots, \bfx_N}$ be a set of training inputs and let $\bfY = \set{\bfy_1, \dots, \bfy_N}$ be a set of training outputs. The objective is to find the parameters $\bfomega$ of a parameterized function $\bfy = f_{\bfomega}(\bfx)$. In Bayesian modelling, we place a prior distribution $p(\bfomega)$ over the parameters, i.e. initial belief of how likely the parameters have generated the observations. We define a likelihood distribution $p(\bfy|\bfx, \bfomega)$ to indicate how likely $\bfy$ is generated by the input $\bfx$ given the parameters $\bfomega$. The posterior distribution over the parameters is defined by
\begin{equation*}
    p(\bfomega|\bfX, \bfY) = \frac{p(\bfY|\bfX, \bfomega)p(\bfomega)}{p(\bfY|\bfX)}
\end{equation*}
Then the predictive distribution of a new test input $\bfx^*$ can be obtained by integrating model parameters
\begin{equation*}
    p(\bfy^*|\bfx^*, \bfX, \bfY) = \int p(\bfy^*|\bfx^*, \bfomega)p(\bfomega|\bfX, \bfY) \dd\bfomega
\end{equation*}
However, the denominator of the posterior $p(\bfY|\bfX) = \int p(\bfY|\bfX, \bfomega)p(\bfomega) \dd\bfomega$ is intractable to obtain analytically. Because true posterior is intractable, in VI, we use an approximating variational distribution $q_\phi(\bfomega|\bfX, \bfY)\approx p_\theta(\bfomega|\bfX, \bfY)$. The optimum is obtained by minimizing the KL divergence between two distributions, leading to a fundamental theorem in VI. Intuitively, minimizing KL divergence is equivalent to maximizing the evidence lower bound (ELBO) w.r.t. the variational parameters $\phi$. 
\begin{theorem}
    \begin{align*}
        &\min_{\phi} \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega|\bfD)} \\
        \iff
        &\max_{\phi} \left\{ \Exp{\log p_{\theta}(\bfD|\bfomega)}{q_{\phi(\bfomega|\bfD)}}-\KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega)} \right\}
    \end{align*}
\end{theorem}
\subsection*{First proof}
\begin{proof}
	By the definition of KL divergence, we can obtain
    \begin{align*}
    	&\KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega|\bfD)}\\
    	=\quad 
    	&\Exp{\log q_{\phi}(\bfomega|\bfD)}{q_{\phi}(\bfomega|\bfD)} + \Exp{\log p_{\theta}(\bfD)}{q_{\phi}(\bfomega|\bfD)} - \Exp{\log p_{\theta}(\bfD, \bfomega)}{q_{\phi}(\bfomega|\bfD)}.
    \end{align*}
    Note that $\log p_{\theta}(\bfD)=\Exp{\log p_{\theta}(\bfD)}{q_{\phi}(\bfomega|\bfD)}$
    and by rearranging the equality, we have
    \begin{align*}
    	&\log p_{\theta}(\bfD) \\
    	=\quad 
    	&\Exp{\log p_{\theta}(\bfD, \bfomega)}{q_{\phi}(\bfomega|\bfD)} - \Exp{\log q_{\phi}(\bfomega|\bfD)}{q_{\phi}(\bfomega|\bfD)} + \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega|\bfD)}.
    \end{align*}
    Applying the non-negativity property of the KL divergence i.e. $\KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega|\bfD)} \ge 0$
    we have the following
    \begin{align*}
        \log p_{\theta}(\bfD) 
        &\ge \Exp{\log p_{\theta}(\bfD, \bfomega)}{q_{\phi}(\bfomega|\bfD)} - \Exp{\log q_{\phi}(\bfomega|\bfD)}{q_{\phi}(\bfomega|\bfD)} \\
        &= \Exp{\log p_{\theta}(\bfD|\bfomega)}{q_{\phi}(\bfomega|\bfD)} - \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega)} \\
        &= \ccL
    \end{align*}
\end{proof}
ELBO consists of reconstruction term, which tends to find variational parameters that likely generates the observation, and regularization term, which penalizes the model if it deviates too much from the prior distribution. ELBO is a trade-off between them. 
\subsection*{Second proof}
We can also prove the theorem by using Jensen's inequality. Let $X$ be a random variable and for a convex function $f$, the following inequality holds
\begin{equation}
	f(\Exp{X}{}) \le \Exp{f(X)}{}.
\end{equation}
The inequality is reversed if $f$ is a concave function
\begin{equation}
	f(\Exp{X}{}) \ge \Exp{f(X)}{}.
\end{equation}
\begin{proof}
	By applying importance sampling of approximating variational distribution $q_{\phi}(\bfomega|\bfD)$ to the marginal likelihood $\log p_{\theta}(\bfD)$, we have
    \begin{align*}
        \log p_{\theta}(\bfD) &= \log \int p_{\theta}(\bfD|\bfomega)p_{\theta}(\bfomega)\frac{q_{\phi}(\bfomega|\bfD)}{q_{\phi}(\bfomega|\bfD)} \dd\bfomega \\
        &= \log \Exp{\frac{p_{\theta}(\bfD|\bfomega)p_{\theta}(\bfomega)}{q_{\phi}(\bfomega|\bfD)}}{q_{\phi}(\bfomega|\bfD)}\\
        &\ge \Exp{\log\frac{p_{\theta}(\bfD|\bfomega)p_{\theta}(\bfomega)}{q_{\phi}(\bfomega|\bfD)}}{q_{\phi}(\bfomega|\bfD)}\\
        &= \Exp{\log p_{\theta}(\bfD|\bfomega)}{q_{\phi}(\bfomega|\bfD)} - \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega)} \\
        &= \ccL.
    \end{align*}
\end{proof}
    
\subsection*{Third proof}
We can also have a direct proof by expanding the definition of KL divergence.
\begin{proof}
	\begin{align*}
	    &\min_{\phi} \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega|\bfD)}\\
	    \iff
	    &\min_{\phi}\int q_{\phi}(\bfomega|\bfD)\log\frac{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfD|\bfomega)p_{\theta}(\bfomega)}\dd\bfomega\\
	    \iff
	    &\min_{\phi}\left\{ -\Exp{\log p_{\theta}(\bfD|\bfomega)}{q_{\phi}(\bfomega|\bfD)} + \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega)} \right\} \\
	    \iff
	    &\max_{\phi}\left\{ \Exp{\log p_{\theta}(\bfD|\bfomega)}{q_{\phi}(\bfomega|\bfD)} - \KL{q_{\phi}(\bfomega|\bfD)}{p_{\theta}(\bfomega)} \right\}.
	\end{align*}
\end{proof}
    
\printbibliography

\end{document}